{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "from keras.utils import plot_model\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(args):\n",
    "\tmu, sigma = args\n",
    "\tbatch = K.shape(mu)[0]\n",
    "\tdim = K.int_shape(mu)[1]\n",
    "\teps = K.random_normal(shape=(batch, dim))\n",
    "\treturn mu + K.exp(sigma / 2) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_reconstruction_loss(true, pred):\n",
    "\t# Reconstruction loss\n",
    "\treconstruction_loss = binary_crossentropy(\n",
    "\t    K.flatten(true), K.flatten(pred)) * img_width * img_height\n",
    "\t# KL divergence loss\n",
    "\tkl_loss = 1 + sigma - K.square(mu) - K.exp(sigma)\n",
    "\tkl_loss = K.sum(kl_loss, axis=-1)\n",
    "\tkl_loss *= -0.5\n",
    "\t# Total loss = 50% rec + 50% KL divergence loss\n",
    "\treturn K.mean(reconstruction_loss + kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_mse(x, vae):\n",
    "    pred = vae.predict(x, batch_size = batch_size)\n",
    "    sq = np.square(pred)\n",
    "    mean = np.mean(sq, (1,2,3))\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histrogram(x_train, x_test, anomaly_x_test):\n",
    "    vae = load_model(\n",
    "    'C:\\\\Users\\\\tim.reicheneder\\\\Desktop\\\\Bachelorthesis\\\\impl_final\\\\vae_with_idcard\\\\vae.h5', compile = False)\n",
    "    vae.compile(optimizer = 'adam', loss = kl_reconstruction_loss)\n",
    "\n",
    "    # calc error\n",
    "    fig, ax1 = plt.subplots(1,1, figsize = (8,8))\n",
    "    ax1.hist(model_mse(x_train, vae), alpha = 1.0, label = 'Training data', normed = True)\n",
    "    ax1.hist(model_mse(x_test, vae), alpha = 0.5, label = 'Testing data', normed = True)\n",
    "    ax1.hist(model_mse(anomaly_x_test, vae), alpha = 0.5, label = 'Anomaly data', normed = True)\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('Reconstruction Error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_comparison(x_test, anomaly_x_test):\n",
    "    # load model\n",
    "    vae = load_model(\n",
    "        'C:\\\\Users\\\\tim.reicheneder\\\\Desktop\\\\Bachelorthesis\\\\impl_final\\\\vae_with_idcard\\\\vae.h5', compile = False)\n",
    "    vae.compile(optimizer = 'adam', loss = kl_reconstruction_loss)\n",
    "\n",
    "    # reconstruction\n",
    "    fig, m_axs = plt.subplots(5,4, figsize=(20, 10))\n",
    "    [c_ax.axis('off') for c_ax in m_axs.ravel()]\n",
    "    for i, (axa_in, axa_vae, axt_in, axt_vae) in enumerate(m_axs):\n",
    "        axa_in.imshow(np.squeeze(anomaly_x_test[i]))\n",
    "        axa_in.set_title('Anomaly In')\n",
    "        axa_vae.imshow(vae.predict(anomaly_x_test[i:i+1])[0,:,:,0])\n",
    "        axa_vae.set_title('Anomaly/Reconstructed')\n",
    "        axt_in.imshow(np.squeeze(x_test[i]))\n",
    "        axt_in.set_title('Test In')\n",
    "        axt_vae.imshow(vae.predict(x_test[i:i+1])[0,:,:,0])\n",
    "        axt_vae.set_title('Test Reconstructed')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_latent_space(encoder, data):\n",
    "\tinput_data, target_data = data\n",
    "\tmu, _, _ = encoder.predict(input_data, batch_size = batch_size)\n",
    "\tplt.figure(figsize=(10, 10))\n",
    "\tplt.scatter(mu[:, 0], mu[:, 1], c=target_data)\n",
    "\tplt.xlabel('z - dim 1')\n",
    "\tplt.ylabel('z - dim 2')\n",
    "\tplt.colorbar()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_outliers_colored(target_data):\n",
    "    cols=[]\n",
    "    for val in target_data:\n",
    "        if val == 1:\n",
    "            cols.append('red')\n",
    "        elif val == 7:\n",
    "            cols.append('blue')\n",
    "        else:\n",
    "            cols.append('green')\n",
    "\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_decoded(encoder, decoder, data):\n",
    "\tnum_samples = 15\n",
    "\tfigure = np.zeros(\n",
    "\t    (img_width * num_samples, img_height * num_samples, num_channels))\n",
    "\tgrid_x = np.linspace(-4, 4, num_samples)\n",
    "\tgrid_y = np.linspace(-4, 4, num_samples)[::-1]\n",
    "\tfor i, yi in enumerate(grid_y):\n",
    "\t\tfor j, xi in enumerate(grid_x):\n",
    "\t\t\tz_sample = np.array([[xi, yi]])\n",
    "\t\t\tx_decoded = decoder.predict(z_sample)\n",
    "\t\t\tdigit = x_decoded[0].reshape(img_width, img_height, num_channels)\n",
    "\t\t\tfigure[i * img_width: (i + 1) * img_width,\n",
    "\t\t\t\t\tj * img_height: (j + 1) * img_height] = digit\n",
    "\tplt.figure(figsize=(10, 10))\n",
    "\tstart_range = img_width // 2\n",
    "\tend_range = num_samples * img_width + start_range + 1\n",
    "\tpixel_range = np.arange(start_range, end_range, img_width)\n",
    "\tsample_range_x = np.round(grid_x, 1)\n",
    "\tsample_range_y = np.round(grid_y, 1)\n",
    "\tplt.xticks(pixel_range, sample_range_x)\n",
    "\tplt.yticks(pixel_range, sample_range_y)\n",
    "\tplt.xlabel('z - dim 1')\n",
    "\tplt.ylabel('z - dim 2')\n",
    "\t# matplotlib.pyplot.imshow() needs a 2D array, or a 3D array with the third dimension being of shape 3 or 4!\n",
    "\t# So reshape if necessary\n",
    "\tfig_shape = np.shape(figure)\n",
    "\tif fig_shape[2] == 1:\n",
    "\t\tfigure = figure.reshape((fig_shape[0], fig_shape[1]))\n",
    "\t# Show image\n",
    "\tplt.imshow(figure)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAndPrepareImages(path) :\n",
    "    print('data prep begins')\n",
    "    img_array = []\n",
    "    for filename in os.listdir(path):\n",
    "        temp = cv2.imread(path + filename)\n",
    "        temp = cv2.cvtColor(temp, cv2.COLOR_BGR2GRAY)\n",
    "        temp = cv2.resize(temp, (800, 600))\n",
    "        img_array.append(temp)\n",
    "\n",
    "    img_array = np.array(img_array, dtype=\"uint8\")\n",
    "\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config // hyperparameter\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "latent_dim = 2\n",
    "\n",
    "img_width, img_height = 800, 600\n",
    "num_channels = 1\n",
    "input_shape = (img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    train_datagen = ImageDataGenerator( \n",
    "    rescale = 1. / 255,  \n",
    "    brightness_range=[0.7, 1.3],\n",
    "    validation_split=0.2)\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale = 1. / 255)\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        \"C:\\\\Users\\\\tim.reicheneder\\\\Desktop\\\\Bachelorthesis\\\\impl_final\\\\pictures_idcard\",\n",
    "        target_size=(800, 600),\n",
    "        color_mode='grayscale',\n",
    "        batch_size=batch_size,\n",
    "        class_mode='input',\n",
    "        subset='training')\n",
    "    \n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        \"C:\\\\Users\\\\tim.reicheneder\\\\Desktop\\\\Bachelorthesis\\\\impl_final\\\\pictures_idcard\",\n",
    "        target_size=(800, 600),\n",
    "        color_mode='grayscale',\n",
    "        batch_size=batch_size,\n",
    "        class_mode='input',\n",
    "        subset='validation')\n",
    "    \n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    i = Input(shape=input_shape, name='encoder_input')\n",
    "    \n",
    "    en = Conv2D(filters=num_channels, kernel_size=2, padding='same', activation='relu')(i)\n",
    "    en = BatchNormalization()(en)\n",
    "    \n",
    "    en = Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu')(en)\n",
    "    en = BatchNormalization()(en)\n",
    "    \n",
    "    en = Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')(en)\n",
    "    en = BatchNormalization()(en)\n",
    "\n",
    "    en = Conv2D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')(en)\n",
    "    en_n = BatchNormalization()(en)\n",
    "    \n",
    "    en = Flatten()(en_n)\n",
    "    \n",
    "    en = Dense(128, activation='relu')(en)\n",
    "    en = BatchNormalization()(en)\n",
    "    \n",
    "    mu = Dense(latent_dim, name='latent_mu')(en)\n",
    "    sigma = Dense(latent_dim, name='latent_sigma')(en)\n",
    "\n",
    "    # get conv shape\n",
    "    conv_shape = K.int_shape(en_n)\n",
    "\n",
    "    # Use reparameterization trick to ensure correct gradient\n",
    "    z = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([mu, sigma])\n",
    "\n",
    "    # Instantiate encoder\n",
    "    encoder = Model(i, [mu, sigma, z], name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    # decoder\n",
    "    de_i = Input(shape=(latent_dim, ), name='decoder_input')\n",
    "\n",
    "    de = Dense(conv_shape[1] * conv_shape[2] * conv_shape[3], activation='relu')(de_i)\n",
    "    de = BatchNormalization()(de)\n",
    "    \n",
    "    de = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(de)\n",
    "    \n",
    "    de = Conv2DTranspose(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')(de)\n",
    "    de = BatchNormalization()(de)\n",
    "    \n",
    "    de = Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')(de)\n",
    "    de = BatchNormalization()(de)\n",
    "    \n",
    "    de = Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu')(de)\n",
    "    de = BatchNormalization()(de)\n",
    "    \n",
    "    de = Conv2DTranspose(filters=num_channels, kernel_size=3, activation='sigmoid', padding='same', name='decoder_output')(de)\n",
    "\n",
    "    # Instantiate decoder\n",
    "    decoder = Model(de_i, de, name='decoder')\n",
    "    decoder.summary()\n",
    "\n",
    "    # VAE\n",
    "    vae_outputs = decoder(encoder(i)[2])\n",
    "    vae = Model(i, vae_outputs, name='vae')\n",
    "    vae.summary()\n",
    "    \n",
    "    return encoder, decoder, vae, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_and_train(encoder, decoder, vae):\n",
    "    vae.compile(optimizer='adam', loss=kl_reconstruction_loss)\n",
    "\n",
    "    # Train autoencoder\n",
    "    logdir = 'C:\\\\Users\\\\tim.reicheneder\\\\Desktop\\\\Bachelorthesis\\\\impl_final\\\\vae_idcard_color\\\\tmp\\\\vae_' + datetime.now().strftime(\"%H_%M\")\n",
    "    \n",
    "    print('train_samples = ' + train_generator.samples)\n",
    "    \n",
    "    vae.fit_generator(\n",
    "        generator = train_generator,\n",
    "        steps_per_epoch = train_generator.samples/batch_size,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps = validation_generator.samples/batch_size,\n",
    "        epochs = epochs,\n",
    "        callbacks = [TensorBoard(log_dir= logdir, histogram_freq=1)]) \n",
    "    \n",
    "    vae.save('C:\\\\Users\\\\tim.reicheneder\\\\Desktop\\\\Bachelorthesis\\\\impl_final\\\\vae_idcard_color\\\\vae.h5')\n",
    "\n",
    "    encoder.save('C:\\\\Users\\\\tim.reicheneder\\\\Desktop\\\\Bachelorthesis\\\\impl_final\\\\vae_idcard_color\\\\encoder.h5')\n",
    "\n",
    "    decoder.save('C:\\\\Users\\\\tim.reicheneder\\\\Desktop\\\\Bachelorthesis\\\\impl_final\\\\vae_idcard_color\\\\decoder.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 600, 800, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 600, 800, 1)  5           encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 600, 800, 1)  4           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 300, 400, 32) 320         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 300, 400, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 150, 200, 64) 18496       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 150, 200, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 75, 100, 128) 73856       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 75, 100, 128) 512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 960000)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          122880128   flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "latent_mu (Dense)               (None, 2)            258         batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "latent_sigma (Dense)            (None, 2)            258         batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 2)            0           latent_mu[0][0]                  \n",
      "                                                                 latent_sigma[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 122,974,733\n",
      "Trainable params: 122,974,027\n",
      "Non-trainable params: 706\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 960000)            2880000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 960000)            3840000   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 75, 100, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 150, 200, 128)     147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 150, 200, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 300, 400, 64)      73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 300, 400, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 600, 800, 32)      18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 600, 800, 32)      128       \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 600, 800, 1)       289       \n",
      "=================================================================\n",
      "Total params: 6,961,025\n",
      "Trainable params: 5,040,577\n",
      "Non-trainable params: 1,920,448\n",
      "_________________________________________________________________\n",
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 600, 800, 1)       0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 2), (None, 2), (N 122974733 \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 600, 800, 1)       6961025   \n",
      "=================================================================\n",
      "Total params: 129,935,758\n",
      "Trainable params: 128,014,604\n",
      "Non-trainable params: 1,921,154\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder, vae = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1013 images belonging to 6 classes.\n",
      "Found 252 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, validation_generator = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sigma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-f0da27253b16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcomp_and_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-dd1e0973bccd>\u001b[0m in \u001b[0;36mcomp_and_train\u001b[1;34m(encoder, decoder, vae)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcomp_and_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkl_reconstruction_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Train autoencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlogdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\\\Users\\\\tim.reicheneder\\\\Desktop\\\\Bachelorthesis\\\\impl_final\\\\vae_idcard_color\\\\tmp\\\\vae_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%H_%M\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tim.reicheneder\\desktop\\bachelorthesis\\impl_final\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tim.reicheneder\\desktop\\bachelorthesis\\impl_final\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m#                   loss_weight_2 * output_2_loss_fn(...) +\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;31m#                   layer losses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_total_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;31m# Functions for train, test and predict will\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tim.reicheneder\\desktop\\bachelorthesis\\impl_final\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_prepare_total_loss\u001b[1;34m(self, masks)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                     output_loss = loss_fn(\n\u001b[1;32m--> 692\u001b[1;33m                         y_true, y_pred, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tim.reicheneder\\desktop\\bachelorthesis\\impl_final\\venv\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mscope_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lambda'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'<lambda>'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             return losses_utils.compute_weighted_loss(\n\u001b[0;32m     73\u001b[0m                 losses, sample_weight, reduction=self.reduction)\n",
      "\u001b[1;32mc:\\users\\tim.reicheneder\\desktop\\bachelorthesis\\impl_final\\venv\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mLoss\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \"\"\"\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-bb4d8873388b>\u001b[0m in \u001b[0;36mkl_reconstruction_loss\u001b[1;34m(true, pred)\u001b[0m\n\u001b[0;32m      4\u001b[0m \t    K.flatten(true), K.flatten(pred)) * img_width * img_height\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m# KL divergence loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mkl_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msigma\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mkl_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkl_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mkl_loss\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sigma' is not defined"
     ]
    }
   ],
   "source": [
    "comp_and_train(encoder, decoder, vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
